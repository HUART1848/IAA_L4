{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43819f57-439c-4893-988e-2e89b187f258",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LAB 4: Quantization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "753cb856-a2b7-47e9-8357-e56621808bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchinfo\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b2b5700-8460-41de-b1cc-93b16329f553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff03bd6-8c2d-4069-95f7-9c3a8fd7e65e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47864d32-e98d-44f9-8475-6c8576448fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e07fe3-f152-4aed-93c2-91db99d7300a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d306cb41-2a1d-4491-9a6b-0ae0853cb586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, bias=False)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, bias=False)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120, bias=False)\n",
    "        self.fc2 = nn.Linear(120, 84, bias=False)\n",
    "        self.fc3 = nn.Linear(84, 10, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896cfb2f-42c9-4b75-9576-4256afcbeee4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89d6dd1d-42b0-4cd2-9282-ecbbb6a46880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model: nn.Module, dataloader: DataLoader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "def test(model: nn.Module, dataloader: DataLoader, max_samples=None) -> float:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    n_inferences = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            #print(images.size())\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if max_samples:\n",
    "                n_inferences += images.shape[0]\n",
    "                if n_inferences > max_samples:\n",
    "                    break\n",
    "    \n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2921b04b-197c-4a12-8f62-205e98b5766c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.196\n",
      "[1,  4000] loss: 1.937\n",
      "[1,  6000] loss: 1.727\n",
      "[1,  8000] loss: 1.643\n",
      "[1, 10000] loss: 1.560\n",
      "[1, 12000] loss: 1.522\n",
      "[2,  2000] loss: 1.458\n",
      "[2,  4000] loss: 1.413\n",
      "[2,  6000] loss: 1.397\n",
      "[2,  8000] loss: 1.391\n",
      "[2, 10000] loss: 1.359\n",
      "[2, 12000] loss: 1.322\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(net, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "958a8a88-d88b-4b90-812c-f831f4138721",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 51.84%\n"
     ]
    }
   ],
   "source": [
    "score = test(net, testloader)\n",
    "print('Accuracy of the network on the test images: {}%'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb1a47-7f6c-42f4-9f8d-94d353a18d62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Visualization of weights distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0118f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_params(model):\n",
    "    params = torch.Tensor()\n",
    "    for param in net.parameters():\n",
    "        params = torch.cat([params, param.flatten().detach()])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02411bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_report(params):\n",
    "    mean = params.mean()\n",
    "    std = params.std()\n",
    "\n",
    "    print(f\"n: {len(params)}\")\n",
    "    print(f\"min: {params.min()}\")\n",
    "    print(f\"max: {params.max()}\")\n",
    "    print(f\"3-sigma range: [{mean - 3 * std}, {mean + 3 * std}]\")\n",
    "\n",
    "    plt.hist(params, bins=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62d86d23-a105-4104-93a8-995b48a1c40f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 61770\n",
      "min: -0.6724898219108582\n",
      "max: 0.658996045589447\n",
      "3-sigma range: [-0.16994927893392742, 0.16403681715019047]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = all_params(net)\n",
    "params_report(params.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b9e7f-ee74-400b-b0ec-1d348e90f796",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Post training quantization: Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d63852b-80d2-469a-ac00-6330526b8e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# A convenience function which we use to copy CNNs\n",
    "def copy_model(model: nn.Module) -> nn.Module:\n",
    "    result = deepcopy(model)\n",
    "    if hasattr(model, 'input_activations'):\n",
    "        result.input_activations = deepcopy(model.input_activations)\n",
    "\n",
    "    for result_layer, original_layer in zip(result.children(), model.children()):\n",
    "        if isinstance(result_layer, nn.Conv2d) or isinstance(result_layer, nn.Linear):\n",
    "            if hasattr(original_layer.weight, 'scale'):\n",
    "                result_layer.weight.scale = deepcopy(original_layer.weight.scale)\n",
    "            if hasattr(original_layer, 'activations'):\n",
    "                result_layer.activations = deepcopy(original_layer.activations)\n",
    "            if hasattr(original_layer, 'output_scale'):\n",
    "                result_layer.output_scale = deepcopy(original_layer.output_scale)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ea6bd80-4d54-472d-ae7e-d5e25928a5e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def quantized_weights(weights: torch.Tensor, target_range: float = 127.0) -> Tuple[torch.Tensor, float]:\n",
    "    '''\n",
    "    Quantize the weights so that all values are integers between -128 and 127.\n",
    "    You may want to use the total range, 3-sigma range, or some other range when\n",
    "    deciding just what factors to scale the float32 values by.\n",
    "\n",
    "    Parameters:\n",
    "    weights (Tensor): The unquantized weights\n",
    "    target_range (float): The desired range for the quantized values. Default is 127.0.\n",
    "\n",
    "    Returns:\n",
    "    (Tensor, float): A tuple with the following elements:\n",
    "                        * The weights in quantized form, where every value is an integer between -128 and 127.\n",
    "                          The \"dtype\" will still be \"float\", but the values themselves should all be integers.\n",
    "                        * The scaling factor that your weights were multiplied by.\n",
    "                          This value does not need to be an 8-bit integer.\n",
    "    '''\n",
    "    scale = 127 / target_range\n",
    "    params = torch.clamp(weights, -target_range, target_range)\n",
    "    params = params * scale\n",
    "    params = torch.round(params)\n",
    "    \n",
    "    return params, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee93cc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6590)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(127.)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(params.max())\n",
    "quantized_weights(weights=params, target_range=3 * params.std())[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4466f089-0775-4495-bc2c-0f626ba2250e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_q2 = copy_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24b54c30-a593-4fcc-8de8-1fa6568822e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min tensor(-0.6725, grad_fn=<MinBackward1>)\n",
      "max tensor(0.6590, grad_fn=<MaxBackward1>)\n",
      "3std tensor(0.5970, grad_fn=<MulBackward0>)\n",
      "\n",
      "min tensor(-0.3790, grad_fn=<MinBackward1>)\n",
      "max tensor(0.3896, grad_fn=<MaxBackward1>)\n",
      "3std tensor(0.3316, grad_fn=<MulBackward0>)\n",
      "\n",
      "min tensor(-0.2309, grad_fn=<MinBackward1>)\n",
      "max tensor(0.2701, grad_fn=<MaxBackward1>)\n",
      "3std tensor(0.1289, grad_fn=<MulBackward0>)\n",
      "\n",
      "min tensor(-0.2275, grad_fn=<MinBackward1>)\n",
      "max tensor(0.2318, grad_fn=<MaxBackward1>)\n",
      "3std tensor(0.1929, grad_fn=<MulBackward0>)\n",
      "\n",
      "min tensor(-0.3743, grad_fn=<MinBackward1>)\n",
      "max tensor(0.4100, grad_fn=<MaxBackward1>)\n",
      "3std tensor(0.3847, grad_fn=<MulBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def quantize_layer_weights(model: nn.Module):\n",
    "    for layer in model.children():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            q_layer_data, scale = quantized_weights(layer.weight.data, layer.weight.max())\n",
    "\n",
    "            print(\"min\", layer.weight.min())\n",
    "            print(\"max\", layer.weight.max())\n",
    "            print(\"3std\", 3 * layer.weight.std(), end=\"\\n\\n\")\n",
    "            q_layer_data = q_layer_data.to(device)\n",
    "\n",
    "            layer.weight.data = q_layer_data\n",
    "            layer.weight.scale = scale\n",
    "\n",
    "            if (q_layer_data < -128).any() or (q_layer_data > 127).any():\n",
    "                raise Exception(\"Quantized weights of {} layer include values out of bounds for an 8-bit signed integer\".format(layer.__class__.__name__))\n",
    "            if (q_layer_data != q_layer_data.round()).any():\n",
    "                raise Exception(\"Quantized weights of {} layer include non-integer values\".format(layer.__class__.__name__))\n",
    "\n",
    "quantize_layer_weights(net_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85fb2ee7-7f50-4226-b93a-aa9d3f312d89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network after quantizing all weights: 51.77%\n"
     ]
    }
   ],
   "source": [
    "score = test(net_q2, testloader)\n",
    "print('Accuracy of the network after quantizing all weights: {}%'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1a0ee2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [1, 10]                   --\n",
      "├─Conv2d: 1-1                            [6, 28, 28]               450\n",
      "├─MaxPool2d: 1-2                         [6, 14, 14]               --\n",
      "├─Conv2d: 1-3                            [16, 10, 10]              2,400\n",
      "├─MaxPool2d: 1-4                         [16, 5, 5]                --\n",
      "├─Linear: 1-5                            [1, 120]                  48,000\n",
      "├─Linear: 1-6                            [1, 84]                   10,080\n",
      "├─Linear: 1-7                            [1, 10]                   840\n",
      "==========================================================================================\n",
      "Total params: 61,770\n",
      "Trainable params: 61,770\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.52\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.25\n",
      "Estimated Total Size (MB): 0.31\n",
      "==========================================================================================\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [1, 10]                   --\n",
      "├─Conv2d: 1-1                            [6, 28, 28]               450\n",
      "├─MaxPool2d: 1-2                         [6, 14, 14]               --\n",
      "├─Conv2d: 1-3                            [16, 10, 10]              2,400\n",
      "├─MaxPool2d: 1-4                         [16, 5, 5]                --\n",
      "├─Linear: 1-5                            [1, 120]                  48,000\n",
      "├─Linear: 1-6                            [1, 84]                   10,080\n",
      "├─Linear: 1-7                            [1, 10]                   840\n",
      "==========================================================================================\n",
      "Total params: 61,770\n",
      "Trainable params: 61,770\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.52\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.25\n",
      "Estimated Total Size (MB): 0.31\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "## Print the size of the model\n",
    "## ADD YOUR CODE HERE\n",
    "\n",
    "print(torchinfo.summary(net, input_size=(3, 32, 32)))\n",
    "print(torchinfo.summary(net_q2, input_size=(3, 32, 32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d69f32-1e68-4ccd-a67e-f2ca25e7b4df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Quantization activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf09a6e-e24d-41a3-b3f6-f1a6c831e77b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def register_activation_profiling_hooks(model: Net):\n",
    "    model.input_activations = np.empty(0)\n",
    "    model.conv1.activations = np.empty(0)\n",
    "    model.conv2.activations = np.empty(0)\n",
    "    model.fc1.activations = np.empty(0)\n",
    "    model.fc2.activations = np.empty(0)\n",
    "    model.fc3.activations = np.empty(0)\n",
    "\n",
    "    model.profile_activations = True\n",
    "\n",
    "    def conv1_activations_hook(layer, x, y):\n",
    "        if model.profile_activations:\n",
    "            model.input_activations = np.append(model.input_activations, x[0].cpu().view(-1))\n",
    "    model.conv1.register_forward_hook(conv1_activations_hook)\n",
    "\n",
    "    def conv2_activations_hook(layer, x, y):\n",
    "        if model.profile_activations:\n",
    "            model.conv1.activations = np.append(model.conv1.activations, x[0].cpu().view(-1))\n",
    "    model.conv2.register_forward_hook(conv2_activations_hook)\n",
    "\n",
    "    def fc1_activations_hook(layer, x, y):\n",
    "        if model.profile_activations:\n",
    "            model.conv2.activations = np.append(model.conv2.activations, x[0].cpu().view(-1))\n",
    "    model.fc1.register_forward_hook(fc1_activations_hook)\n",
    "\n",
    "    def fc2_activations_hook(layer, x, y):\n",
    "        if model.profile_activations:\n",
    "            model.fc1.activations = np.append(model.fc1.activations, x[0].cpu().view(-1))\n",
    "    model.fc2.register_forward_hook(fc2_activations_hook)\n",
    "\n",
    "    def fc3_activations_hook(layer, x, y):\n",
    "        if model.profile_activations:\n",
    "            model.fc2.activations = np.append(model.fc2.activations, x[0].cpu().view(-1))\n",
    "            model.fc3.activations = np.append(model.fc3.activations, y[0].cpu().view(-1))\n",
    "    model.fc3.register_forward_hook(fc3_activations_hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af1254-c4d3-4015-ba4b-83b333776de7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_q3 = copy_model(net)\n",
    "register_activation_profiling_hooks(net_q3)\n",
    "\n",
    "# Run through the training dataset again while profiling the input and output activations this time\n",
    "# We don't actually have to perform gradient descent for this, so we can use the \"test\" function\n",
    "test(net_q3, trainloader, max_samples=400)\n",
    "net_q3.profile_activations = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf28b1-5deb-4dce-a118-a59154fec6a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Visualize activation distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc85f64-c27c-41f6-8449-80f4120f84ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE: to plot activation distribution\n",
    "# You can access the activation matrice like this:\n",
    "# net_q3.input_activations\n",
    "# net_q3.conv1_activations\n",
    "# ...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07aebf0-0287-4ef0-b86a-b4a24579d23b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Quantize activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e1d4d-206d-424c-8d47-9961410d168f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class NetQuantized(nn.Module):\n",
    "    def __init__(self, net_with_weights_quantized: nn.Module):\n",
    "        super(NetQuantized, self).__init__()\n",
    "        \n",
    "        net_init = copy_model(net_with_weights_quantized)\n",
    "\n",
    "        self.conv1 = net_init.conv1\n",
    "        self.pool = net_init.pool\n",
    "        self.conv2 = net_init.conv2\n",
    "        self.fc1 = net_init.fc1\n",
    "        self.fc2 = net_init.fc2\n",
    "        self.fc3 = net_init.fc3\n",
    "\n",
    "        for layer in self.conv1, self.conv2, self.fc1, self.fc2, self.fc3:\n",
    "            def pre_hook(l, x):\n",
    "                x = x[0]\n",
    "                if (x < -128).any() or (x > 127).any():\n",
    "                    raise Exception(\"Input to {} layer is out of bounds for an 8-bit signed integer\".format(l.__class__.__name__))\n",
    "                if (x != x.round()).any():\n",
    "                    raise Exception(\"Input to {} layer has non-integer values\".format(l.__class__.__name__))\n",
    "\n",
    "            layer.register_forward_pre_hook(pre_hook)\n",
    "\n",
    "        # Calculate the scaling factor for the initial input to the CNN\n",
    "        self.input_activations = net_with_weights_quantized.input_activations\n",
    "        self.input_scale = NetQuantized.quantize_initial_input(self.input_activations)\n",
    "\n",
    "        # Calculate the output scaling factors for all the layers of the CNN\n",
    "        preceding_layer_scales = []\n",
    "        for layer in self.conv1, self.conv2, self.fc1, self.fc2, self.fc3:\n",
    "            layer.output_scale = NetQuantized.quantize_activations(layer.activations, layer.weight.scale, self.input_scale, preceding_layer_scales)\n",
    "            preceding_layer_scales.append((layer.weight.scale, layer.output_scale))\n",
    "\n",
    "    @staticmethod\n",
    "    def quantize_initial_input(initial_input: np.ndarray) -> float:\n",
    "        '''\n",
    "        Calculate a scaling factor for the images that are input to the first layer of the CNN.\n",
    "\n",
    "        Parameters:\n",
    "        initial_input (ndarray): The values of all the pixels which were part of the input image during training\n",
    "\n",
    "        Returns:\n",
    "        float: A scaling factor that the input should be multiplied by before being fed into the first layer.\n",
    "               This value does not need to be an 8-bit integer.\n",
    "        '''\n",
    "\n",
    "        # ADD your code here\n",
    "\n",
    "        return scale_factor\n",
    "\n",
    "    @staticmethod\n",
    "    def quantize_activations(activations: np.ndarray, s_w: float, s_initial_input: float, ns: List[Tuple[float, float]]) -> float:\n",
    "        '''\n",
    "        Calculate a scaling factor to multiply the output of a layer by.\n",
    "\n",
    "        Parameters:\n",
    "        activations (ndarray): The values of all the pixels which have been output by this layer during training\n",
    "        s_w (float): The scale by which the weights of this layer were multiplied as part of the \"quantize_weights\" function you wrote earlier\n",
    "        s_initial_input (float): The scale by which the initial input to the neural network was multiplied\n",
    "        ns ([(float, float)]): A list of tuples, where each tuple represents the \"weight scale\" and \"output scale\" (in that order) for every preceding layer\n",
    "\n",
    "        Returns:\n",
    "        float: A scaling factor that the layer output should be multiplied by before being fed into the next layer.\n",
    "               This value does not need to be an 8-bit integer.\n",
    "        '''\n",
    "        # ADD YOUR CODE HERE\n",
    "       \n",
    "        return scale\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # You can access the output activation scales like this:\n",
    "        #   fc1_output_scale = self.fc1.output_scale\n",
    "        # You don't need to quantize Relu outputs \n",
    "\n",
    "        # ADD YOUR CODE HERE (Replace the ...)\n",
    "        # ...\n",
    "        x = self.pool(F.relu(x))\n",
    "        x = torch.clamp(x, min=-128, max=127)\n",
    "        # ...\n",
    "        x = self.pool(F.relu(x))\n",
    "        x = torch.clamp(x, min=-128, max=127)\n",
    "        # ...\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        # ...\n",
    "        x = F.relu(x)\n",
    "        # ...\n",
    "        x = F.relu(x)\n",
    "        # ...\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fbe535-797c-402d-a0a3-11e7d6c3408c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge the information from net_q2 and net_q3 together\n",
    "\n",
    "net_init = copy_model(net_q2)\n",
    "net_init.input_activations = deepcopy(net_q3.input_activations)\n",
    "for layer_init, layer_q3 in zip(net_init.children(), net_q3.children()):\n",
    "    if isinstance(layer_init, nn.Conv2d) or isinstance(layer_init, nn.Linear):\n",
    "        layer_init.activations = deepcopy(layer_q3.activations)\n",
    "\n",
    "net_quantized = NetQuantized(net_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722eb2d-bb2e-44d6-bbd0-b6bbd0db9329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score = test(net_quantized, testloader)\n",
    "print('Accuracy of the network after quantizing both weights and activations: {}%'.format(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
